import torch
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import get_linear_schedule_with_warmup, MobileBertForSequenceClassification, MobileBertTokenizer
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from tqdm import tqdm

# 1. ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… Using device:", device)

# 2. ë°ì´í„° ë¡œë“œ
df = pd.read_csv("news_sentiment_labeled.csv", encoding="cp949")  # ì´ê±´ í•œê¸€ì´ë¼ cp949
data_X = list(df['sentence'].astype(str).values)
labels = df['sentiment'].values

# 3. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
train_texts, val_texts, train_labels, val_labels = train_test_split(
    data_X, labels, test_size=0.2, random_state=42
)

# 4. í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased', do_lower_case=True)

# 5. í† í°í™”
train_inputs = tokenizer(train_texts, truncation=True, max_length=256, padding="max_length", return_tensors="pt")
val_inputs = tokenizer(val_texts, truncation=True, max_length=256, padding="max_length", return_tensors="pt")

train_input_ids = train_inputs['input_ids']
train_attention_mask = train_inputs['attention_mask']

val_input_ids = val_inputs['input_ids']
val_attention_mask = val_inputs['attention_mask']

# 6. TensorDataset êµ¬ì„±
batch_size = 8

train_dataset = TensorDataset(train_input_ids, train_attention_mask, torch.tensor(train_labels))
val_dataset = TensorDataset(val_input_ids, val_attention_mask, torch.tensor(val_labels))

train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)
val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)

# 7. ëª¨ë¸ ì •ì˜
model = MobileBertForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=3)
model.to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*4)

# 8. í•™ìŠµ ë£¨í”„
epochs = 4
for epoch in range(epochs):
    print(f"\nğŸš€ Epoch {epoch+1}/{epochs}")

    # Train
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc="Training"):
        b_input_ids, b_attention_mask, b_labels = [b.to(device) for b in batch]

        model.zero_grad()
        outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)
        loss = outputs.loss
        loss.backward()
        total_loss += loss.item()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

    avg_train_loss = total_loss / len(train_loader)
    print(f"âœ… Average Training Loss: {avg_train_loss:.4f}")

    # Validation
    model.eval()
    preds, true_labels = [], []

    for batch in tqdm(val_loader, desc="Validating"):
        b_input_ids, b_attention_mask, b_labels = [b.to(device) for b in batch]

        with torch.no_grad():
            outputs = model(b_input_ids, attention_mask=b_attention_mask)

        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)

        preds.extend(predictions.cpu().numpy())
        true_labels.extend(b_labels.cpu().numpy())

    val_accuracy = np.mean(np.array(preds) == np.array(true_labels))
    print(f"ğŸ¯ Validation Accuracy: {val_accuracy:.4f}")

# 9. ëª¨ë¸ ì €ì¥
model.save_pretrained("mobilebert_finance_news")
tokenizer.save_pretrained("mobilebert_finance_news")
torch.save(model.state_dict(), "mobilebert_finance_news.pt")
print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ.")
