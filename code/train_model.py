import torch
import pandas as pd
import numpy as np
from transformers import MobileBertForSequenceClassification, MobileBertTokenizer
from tqdm import tqdm

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… Using device:", device)

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("news_sentiment_labeled.csv", encoding="cp949")

data_X = list(df['sentence'].astype(str).values)
labels = df['sentiment'].astype(int).values  # ğŸ”¥ ìˆ˜ì •í•œ ë¶€ë¶„!

# 2. í† í¬ë‚˜ì´ì € ë¡œë“œ ë° í† í°í™”
tokenizer = MobileBertTokenizer.from_pretrained("mobilebert_finance_news")
inputs = tokenizer(data_X, truncation=True, max_length=256, padding="max_length", return_tensors="pt")

input_ids = inputs['input_ids']
attention_mask = inputs['attention_mask']

# 3. í‰ê°€ìš© DataLoader êµ¬ì„±
batch_size = 4
test_data = torch.utils.data.TensorDataset(input_ids, attention_mask, torch.tensor(labels))
test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)

# 4. ëª¨ë¸ ë¡œë“œ
model = MobileBertForSequenceClassification.from_pretrained("google/mobilebert-uncased", num_labels=3)
model.load_state_dict(torch.load("mobilebert_finance_news.pt", map_location=device))
model.to(device)
model.eval()

# 5. ì˜ˆì¸¡ ìˆ˜í–‰
test_pred, test_true = [], []

for batch in tqdm(test_dataloader, desc="ğŸ“Š ëª¨ë¸ ì˜ˆì¸¡ ì¤‘"):
    batch_ids, batch_mask, batch_labels = [b.to(device) for b in batch]
    with torch.no_grad():
        output = model(batch_ids, attention_mask=batch_mask)
    logits = output.logits
    preds = torch.argmax(logits, dim=1)
    test_pred.extend(preds.cpu().numpy())
    test_true.extend(batch_labels.cpu().numpy())

# 6. ì •í™•ë„ ì¶œë ¥
accuracy = np.mean(np.array(test_pred) == np.array(test_true))
print(f"\nâœ… ë‰´ìŠ¤ ê°ì„± ë¶„ë¥˜ ì •í™•ë„: {accuracy:.4f}")
